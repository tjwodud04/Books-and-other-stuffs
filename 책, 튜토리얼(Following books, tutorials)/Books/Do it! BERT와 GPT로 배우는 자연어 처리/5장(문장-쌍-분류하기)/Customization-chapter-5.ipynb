{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9620fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 말뭉치 클래스\n",
    "import os\n",
    "from ratsnlp.nlpbook.classification import ClassificationExample\n",
    "\n",
    "class CustomNLICorpus:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_examples(self, data_root_path, mode):\n",
    "        data_fpath = os.path.join(data_root_path, f\"{mode}.txt\")\n",
    "        lines = open(data_fpath, \"r\", encoding=\"utf-8\").readlines()\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            text_a, text_b, label = line\n",
    "            examples.append(ClassificationExample(text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [\"함의\", \"모순\", \"중립\"]\n",
    "\n",
    "    @property\n",
    "    def num_labels(self):\n",
    "        return len(self.get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae4c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 데이터 로딩 및 전처리\n",
    "from ratsnlp.nlpbook.classification import ClassificationDataset\n",
    "\n",
    "corpus = CustomNLICorpus()\n",
    "train_dataset = ClassificationDataset(\n",
    "\targs=args,\n",
    "\tcorpus=corpus,\n",
    "\ttokenizer=tokenier,\n",
    "\tmode=\"train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e1506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFICATIONDATASET 클래스\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers import PreTrainedTokenizer\n",
    "from ratsnlp.nlpbook.classification.arguments import ClassificationTrainArguments\n",
    "from ratsnlp.nlpbook.classification import _convert_examples_to_classification_features\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            args: ClassificationTrainArguments,\n",
    "            tokenizer: PreTrainedTokenizer,\n",
    "            corpus,\n",
    "            mode: Optional[str] = \"train\",\n",
    "            convert_examples_to_features_fn=_convert_examples_to_classification_features,\n",
    "    ):\n",
    "\n",
    "            self.corpus = corpus\n",
    "\n",
    "                examples = self.corpus.get_examples(corpus_path, mode)\n",
    "                self.features = convert_examples_to_features_fn(\n",
    "                    examples,\n",
    "                    tokenizer,\n",
    "                    args,\n",
    "                    label_list=self.corpus.get_labels(),\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.features[i]\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.corpus.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e17c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 모델 사용하기\n",
    "from ratsnlp.nlpbook.classification import ClassificationTrainArguments\n",
    "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "args = ClassificationTrainArguments(\n",
    "    pretrained_model_name=\"bert-base-uncased\",\n",
    ")\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    do_lower_case=False,\n",
    ")\n",
    "pretrained_model_config = BertConfig.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    ")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    config=pretrained_model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a94ceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 분류 태스크 클래스\n",
    "from transformers import PreTrainedModel\n",
    "from transformers.optimization import AdamW\n",
    "from ratsnlp.nlpbook.metrics import accuracy\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch.optim.lr_scheduler import ExponentialLR, CosineAnnealingWarmRestarts\n",
    "from ratsnlp.nlpbook.classification.arguments import ClassificationTrainArguments\n",
    "\n",
    "class ClassificationTask(LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 model: PreTrainedModel,\n",
    "                 args: ClassificationTrainArguments,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.args.optimizer == 'AdamW':\n",
    "            optimizer = AdamW(self.parameters(), lr=self.args.learning_rate)\n",
    "        else:\n",
    "            raise NotImplementedError('Only AdamW is Supported!')\n",
    "        if self.args.lr_scheduler == 'cos':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)\n",
    "        elif self.args.lr_scheduler == 'exp':\n",
    "            scheduler = ExponentialLR(optimizer, gamma=0.5)\n",
    "        else:\n",
    "            raise NotImplementedError('Only cos and exp lr scheduler is Supported!')\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "        }\n",
    "\n",
    "    def training_step(self, inputs, batch_idx):\n",
    "        # outputs: SequenceClassifierOutput\n",
    "        outputs = self.model(**inputs)\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "        labels = inputs[\"labels\"]\n",
    "        acc = accuracy(preds, labels)\n",
    "        self.log(\"loss\", outputs.loss, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
    "        self.log(\"acc\", acc, prog_bar=True, logger=True, on_step=True, on_epoch=False)\n",
    "        return outputs.loss\n",
    "\n",
    "    def validation_step(self, inputs, batch_idx):\n",
    "        # outputs: SequenceClassifierOutput\n",
    "        outputs = self.model(**inputs)\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "        labels = inputs[\"labels\"]\n",
    "        acc = accuracy(preds, labels)\n",
    "        self.log(\"val_loss\", outputs.loss, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "        return outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTFORSEQUENCECLASSIFICATION\n",
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    " \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "      \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cae52e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
