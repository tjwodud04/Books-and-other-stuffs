{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acefdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSMC 데이터 로딩 및 전처리\n",
    "# 데이터 로딩\n",
    "from ratsnlp.nlpbook.generation import NsmcCorpus\n",
    "\n",
    "corpus = NsmcCorpus()\n",
    "\n",
    "# 데이터 전처리\n",
    "from ratsnlp.nlpbook.generation import GenerationDataset\n",
    "\n",
    "train_dataset = GenerationDataset(\n",
    "    args=args,\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenizer,\n",
    "    mode=\"train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dda6223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSMCCORPUS 클래스\n",
    "import os, csv\n",
    "from ratsnlp.nlpbook.generation.corpus import GenerationExample\n",
    "\n",
    "class NsmcCorpus:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _read_corpus(cls, input_file, quotechar='\"'):\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            return list(csv.reader(f, delimiter=\"\\t\", quotechar=quotechar))\n",
    "\n",
    "    def _create_examples(self, lines):\n",
    "        examples = []\n",
    "        \n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "                \n",
    "            _, review_sentence, sentiment = line\n",
    "            sentiment = \"긍정\" if sentiment == \"1\" else \"부정\"\n",
    "            text = sentiment + \" \" + review_sentence\n",
    "            examples.append(GenerationExample(text=text))\n",
    "            \n",
    "        return examples\n",
    "\n",
    "    def get_examples(self, data_root_path, mode):\n",
    "        data_fpath = os.path.join(data_root_path, f\"ratings_{mode}.txt\")\n",
    "        logger.info(f\"loading {mode} data... LOOKING AT {data_fpath}\")\n",
    "        \n",
    "        return self._create_examples(self._read_corpus(data_fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8857bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 말뭉치 클래스\n",
    "import os\n",
    "from ratsnlp.nlpbook.generation.corpus import GenerationExample\n",
    "\n",
    "class NewsCorpus:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_examples(self, data_root_path, mode):\n",
    "        data_fpath = os.path.join(data_root_path, f\"{mode}.txt\")\n",
    "        lines = open(data_fpath, \"r\", encoding=\"utf-8\").readlines()\n",
    "        examples = []\n",
    "        \n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "                \n",
    "            text, label = line\n",
    "            sentence = label + \" \" + text \n",
    "            examples.append(GenerationExample(text=sentence))\n",
    "            \n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b4481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 데이터 로딩 및 전처리\n",
    "from ratsnlp.nlpbook.generation import GenerationDataset\n",
    "\n",
    "corpus = NewsCorpus()\n",
    "\n",
    "train_dataset = GenerationDataset(\n",
    "    args=args,\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenier,\n",
    "    mode=\"train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085fee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATIONDATASET 클래스\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers import PreTrainedTokenizer\n",
    "from ratsnlp.nlpbook.generation.arguments import GenerationTrainArguments\n",
    "from ratsnlp.nlpbook.generation.corpus import _convert_examples_to_generation_features\n",
    "\n",
    "class GenerationDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            args: GenerationTrainArguments,\n",
    "            tokenizer: PreTrainedTokenizerFast,\n",
    "            corpus,\n",
    "            mode: Optional[str] = \"train\",\n",
    "            convert_examples_to_features_fn=_convert_examples_to_generation_features,\n",
    "    ):\n",
    "            self.corpus = corpus\n",
    "            \n",
    "                examples = self.corpus.get_examples(corpus_path, mode)\n",
    "                self.features = convert_examples_to_features_fn(\n",
    "                    examples,\n",
    "                    tokenizer,\n",
    "                    args,\n",
    "                    label_list=self.corpus.get_labels(),\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 모델 사용하기\n",
    "from ratsnlp.nlpbook.generation import GenerationTrainArguments\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "args = GenerationTrainArguments(\n",
    "    pretrained_model_name=\"gpt2\",\n",
    ")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673011ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ratsnlp.nlpbook.generation import GenerationTask\n",
    "\n",
    "task = GenerationTask(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc02afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 생성 태스크 클래스\n",
    "from transformers import PreTrainedModel\n",
    "from transformers.optimization import AdamW\n",
    "from pytorch_lightning import LightningModule\n",
    "from ratsnlp.nlpbook.generation.arguments import GenerationTrainArguments\n",
    "from torch.optim.lr_scheduler import ExponentialLR, CosineAnnealingWarmRestarts\n",
    "\n",
    "class GenerationTask(LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 model: PreTrainedModel,\n",
    "                 args: GenerationTrainArguments,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.args.learning_rate)\n",
    "        scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "        }\n",
    "\n",
    "    def training_step(self, inputs, batch_idx):\n",
    "        # outputs: CausalLMOutputWithCrossAttentions\n",
    "        outputs = self.model(**inputs)\n",
    "        self.log(\"loss\", outputs.loss, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
    "        return outputs.loss\n",
    "\n",
    "    def validation_step(self, inputs, batch_idx):\n",
    "        # outputs: CausalLMOutputWithCrossAttentions\n",
    "        outputs = self.model(**inputs)\n",
    "        self.log(\"val_loss\", outputs.loss, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "        return outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952a89ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2LMHEADMODEL\n",
    "class GPT2LMHeadModel(GPT2PreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.transformer = GPT2Model(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.model_parallel = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        \n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.transformer.first_device)\n",
    "            hidden_states = hidden_states.to(self.lm_head.weight.device)\n",
    "\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        \n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "            cross_attentions=transformer_outputs.cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01704359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a4fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
