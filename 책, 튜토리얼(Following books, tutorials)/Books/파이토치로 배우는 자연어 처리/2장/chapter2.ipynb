{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c8b07ed",
   "metadata": {},
   "source": [
    "2.1 말뭉치, 토큰, 타입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b21dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "841465f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mary', ',', 'do', \"n't\", 'slap', 'the', 'green', 'witch', '.']\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 토큰화\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"Mary, don't slap the green witch.\"\n",
    "\n",
    "print([str(token) for token in nlp(text.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04bdaa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['snow', 'white', 'and', 'the', 'seven', 'degrees', '#makeamoviecold', '@midnight', ':-)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet = u\"Snow White and the Seven Degrees #MakeAMovieCold@midnight:-)\"\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(tweet.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb320b43",
   "metadata": {},
   "source": [
    "2.2 유니그램, 바이그램, 트라이그램, ∙∙∙, n-그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e38b6625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['mary', ',', \"n't\"],\n",
       " [',', \"n't\", 'slap'],\n",
       " [\"n't\", 'slap', 'green'],\n",
       " ['slap', 'green', 'witch'],\n",
       " ['green', 'witch', '.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텍스트에서 n-그램 만들기\n",
    "def n_grams(text, n):\n",
    "    '''\n",
    "    takes tokens or text, returns a list of n-grams\n",
    "    '''\n",
    "    return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "\n",
    "cleaned = ['mary', ',', \"n't\", 'slap', 'green', 'witch', '.']\n",
    "n_grams(cleaned, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e274e",
   "metadata": {},
   "source": [
    "2.3 표제어와 어간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49e2614f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he --> he\n",
      "was --> be\n",
      "running --> run\n",
      "late --> late\n"
     ]
    }
   ],
   "source": [
    "# 표제어 추출: 단어를 표제어로 바꿉니다.\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"he was running late\")\n",
    "\n",
    "for token in doc:\n",
    "    print('{} --> {}'.format(token, token.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59628354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cared ----> care\n",
      "university ----> univers\n",
      "fairly ----> fair\n",
      "easily ----> easili\n",
      "singing ----> sing\n",
      "sings ----> sing\n",
      "sung ----> sung\n",
      "singer ----> singer\n",
      "sportingly ----> sport\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "  \n",
    "words = ['cared','university','fairly','easily','singing','sings','sung','singer','sportingly']\n",
    "  \n",
    "stem_words = []\n",
    "\n",
    "for w in words:\n",
    "    x = snow_stemmer.stem(w)\n",
    "    stem_words.append(x)\n",
    "      \n",
    "for e1,e2 in zip(words,stem_words):\n",
    "    print(e1+' ----> '+e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a79d1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cared ----> care\n",
      "university ----> univers\n",
      "fairly ----> fairli\n",
      "easily ----> easili\n",
      "singing ----> sing\n",
      "sings ----> sing\n",
      "sung ----> sung\n",
      "singer ----> singer\n",
      "sportingly ----> sportingli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "  \n",
    "words = ['cared','university','fairly','easily','singing','sings','sung','singer','sportingly']\n",
    "  \n",
    "stem_words = []\n",
    "\n",
    "for w in words:\n",
    "    x = porter_stemmer.stem(w)\n",
    "    stem_words.append(x)\n",
    "      \n",
    "for e1,e2 in zip(words,stem_words):\n",
    "    print(e1+' ----> '+e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f276b2c",
   "metadata": {},
   "source": [
    "2.5 단어 분류하기: 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa331a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary --> PROPN\n",
      "slapped --> VERB\n",
      "the --> DET\n",
      "green --> ADJ\n",
      "witch --> NOUN\n",
      ". --> PUNCT\n"
     ]
    }
   ],
   "source": [
    "# 품사 태깅\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(u\"Mary slapped the green witch.\")\n",
    "\n",
    "for token in doc:\n",
    "    print('{} --> {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7bc7b8",
   "metadata": {},
   "source": [
    "2.6 청크 나누기와 개체명 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b34cb184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary --> NP\n",
      "the green witch --> NP\n"
     ]
    }
   ],
   "source": [
    "# 명사구(NP) 부문 구문 분석\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(u\"Mary slapped the green witch.\")\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print('{} --> {}'.format(chunk, chunk.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5168fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
