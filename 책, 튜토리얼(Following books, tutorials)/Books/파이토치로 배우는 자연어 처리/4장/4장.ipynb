{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "990441f6",
   "metadata": {},
   "source": [
    "4.1.2 파이토치로 MLP 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93649b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치를 사용한 MLP\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultilayerPerceptron(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size=2, output_size=3, \n",
    "                 num_hidden_layers=1, hidden_activation=nn.Sigmoid):\n",
    "        \"\"\"가중치 초기화\n",
    "\n",
    "        매개변수:\n",
    "            input_size (int): 입력 크기\n",
    "            hidden_size (int): 은닉층 크기\n",
    "            output_size (int): 출력 크기\n",
    "            num_hidden_layers (int): 은닉층 개수\n",
    "            hidden_activation (torch.nn.*): 활성화 함수\n",
    "        \"\"\"\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.module_list = nn.ModuleList()\n",
    "        \n",
    "        interim_input_size = input_size\n",
    "        interim_output_size = hidden_size\n",
    "        \n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.module_list.append(nn.Linear(interim_input_size, interim_output_size))\n",
    "            self.module_list.append(hidden_activation())\n",
    "            interim_input_size = interim_output_size\n",
    "            \n",
    "        self.fc_final = nn.Linear(interim_input_size, output_size)\n",
    "        \n",
    "        self.last_forward_cache = []\n",
    "       \n",
    "    def forward(self, x, apply_softmax=False):\n",
    "        \"\"\"MLP의 정방향 계산\n",
    "        \n",
    "        매개변수:\n",
    "            x_in (torch.Tensor): 입력 데이터 텐서\n",
    "                x_in.shape는 (batch, input_dim)입니다.\n",
    "            apply_softmax (bool): 소프트맥스 함수를 위한 플래그\n",
    "                크로스 엔트로피 손실을 사용하려면 반드시 False로 지정해야 합니다\n",
    "        반환값:\n",
    "            결과 텐서. tensor.shape는 (batch, output_dim)입니다.\n",
    "        \"\"\"\n",
    "        self.last_forward_cache = []\n",
    "        self.last_forward_cache.append(x.to(\"cpu\").numpy())\n",
    "\n",
    "        for module in self.module_list:\n",
    "            x = module(x)\n",
    "            self.last_forward_cache.append(x.to(\"cpu\").data.numpy())\n",
    "            \n",
    "        output = self.fc_final(x)\n",
    "        self.last_forward_cache.append(output.to(\"cpu\").data.numpy())\n",
    "\n",
    "        if apply_softmax:\n",
    "            output = F.softmax(output, dim=1)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5c1e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron(\n",
      "  (module_list): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (fc_final): Linear(in_features=100, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# MLP 객체 생성\n",
    "batch_size = 2 # 한 번에 입력할 샘플 개수\n",
    "input_dim = 3\n",
    "hidden_dim = 100\n",
    "output_dim = 4\n",
    "\n",
    "# 모델 생성\n",
    "mlp = MultilayerPerceptron(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4f3ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "타입: torch.FloatTensor\n",
      "크기: torch.Size([2, 3])\n",
      "값: \n",
      "tensor([[0.4944, 0.5987, 0.6043],\n",
      "        [0.3881, 0.9355, 0.4304]])\n",
      "\n",
      "타입: torch.FloatTensor\n",
      "크기: torch.Size([2, 4])\n",
      "값: \n",
      "tensor([[ 0.5920, -0.1105, -0.1509,  0.0463],\n",
      "        [ 0.6112, -0.1185, -0.1382,  0.0492]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 랜덤한 입력으로 MLP 테스트하기\n",
    "import torch\n",
    "\n",
    "def describe(x):\n",
    "    print(\"타입: {}\".format(x.type()))\n",
    "    print(\"크기: {}\".format(x.shape))\n",
    "    print(\"값: \\n{}\".format(x))\n",
    "\n",
    "x_input = torch.rand(batch_size, input_dim)\n",
    "describe(x_input)\n",
    "\n",
    "print()\n",
    "\n",
    "y_output = mlp(x_input, apply_softmax=False)\n",
    "describe(y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe7d3c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "타입: torch.FloatTensor\n",
      "크기: torch.Size([2, 4])\n",
      "값: \n",
      "tensor([[0.3921, 0.1942, 0.1865, 0.2272],\n",
      "        [0.3961, 0.1909, 0.1872, 0.2258]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# MLP 분류기로 확률 출력하기(apply_softmax=True)\n",
    "y_output = mlp(x_input, apply_softmax=True)\n",
    "describe(y_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fdf154",
   "metadata": {},
   "source": [
    "4.2.1 성씨 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5d2b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SurnameDataset.__getitem__() 구현\n",
    "class SurnameDataset(Dataset):\n",
    "    # [코드 3-14]와 구현이 매우 비슷합니다.\n",
    "\n",
    "    def __getitem__(self, index):           \n",
    "            row = self._target_df.iloc[index]\n",
    "\n",
    "            surname_vector = self._vectorizer.vectorize(row.surname)\n",
    "\n",
    "            nationality_index = self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "\n",
    "            return {'x_surname': surname_vector, 'y_nationality': nationality_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c679f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SurnameVectorizer 구현\n",
    "class SurnameVectorizer(object):\n",
    "    \"\"\" 어휘 사전을 생성하고 관리합니다 \"\"\"\n",
    "    def __init__(self, surname_vocab, nationality_vocab):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            surname_vocab (Vocabulary): 문자를 정수에 매핑하는 Vocabulary 객체\n",
    "            nationality_vocab (Vocabulary): 국적을 정수에 매핑하는 Vocabulary 객체\n",
    "        \"\"\"\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "\n",
    "    def vectorize(self, surname):\n",
    "        \"\"\" 성씨에 대한 원-핫 벡터를 만듭니다\n",
    "\n",
    "        매개변수:\n",
    "            surname (str): 성씨\n",
    "        반환값:\n",
    "            one_hot (np.ndarray): 원-핫 벡터\n",
    "        \"\"\"\n",
    "        vocab = self.surname_vocab\n",
    "        one_hot = np.zeros(len(vocab), dtype=np.float32)\n",
    "        \n",
    "        for token in surname:\n",
    "            one_hot[vocab.lookup_token(token)] = 1\n",
    "\n",
    "        return one_hot\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        \"\"\" 데이터셋 데이터프레임에서 Vectorizer 객체를 만듭니다\n",
    "        \n",
    "        매개변수:\n",
    "            surname_df (pandas.DataFrame): 성씨 데이터셋\n",
    "        반환값:\n",
    "            SurnameVectorizer 객체\n",
    "        \"\"\"\n",
    "        surname_vocab = Vocabulary(unk_token=\"@\")\n",
    "        nationality_vocab = Vocabulary(add_unk=False)\n",
    "\n",
    "        for index, row in surname_df.iterrows():\n",
    "            for letter in row.surname:\n",
    "                surname_vocab.add_token(letter)\n",
    "                \n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "\n",
    "        return cls(surname_vocab, nationality_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de81b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 기반의 SurnameClassifier\n",
    "class SurnameClassifier(nn.Module):\n",
    "    \"\"\" 성씨 분류를 위한 다층 퍼셉트론 \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            input_dim (int): 입력 벡터 크기\n",
    "            hidden_dim (int): 첫 번째 Linear 층의 출력 크기\n",
    "            output_dim (int): 두 번째 Linear 층의 출력 크기\n",
    "        \"\"\"\n",
    "        super(SurnameClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"MLP의 정방향 계산\n",
    "        \n",
    "        매개변수:\n",
    "            x_in (torch.Tensor): 입력 데이터 텐서\n",
    "                x_in.shape는 (batch, input_dim)입니다.\n",
    "            apply_softmax (bool): 소프트맥스 활성화 함수를 위한 플래그\n",
    "                크로스-엔트로피 손실을 사용하려면 False로 지정해야 합니다.\n",
    "        반환값:\n",
    "            결과 텐서. tensor.shape은 (batch, output_dim)입니다.\n",
    "        \"\"\"\n",
    "        intermediate_vector = F.relu(self.fc1(x_in))\n",
    "        prediction_vector = self.fc2(intermediate_vector)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6136aa0",
   "metadata": {},
   "source": [
    "4.2.4 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c8d0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 기반의 성씨 분류기를 위한 하이퍼파라미터와 프로그램 설정\n",
    "args = Namespace(\n",
    "    # 날짜와 경로 정보\n",
    "    surname_csv=\"data/surnames/surnames_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/ch4/surname_mlp\",\n",
    "    # 모델 하이퍼파라미터\n",
    "    hidden_dim=300,\n",
    "    # 훈련 하이퍼파라미터\n",
    "    seed=1337,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=64,\n",
    "    # 실행 옵션\n",
    "    cuda=False,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6325356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 모델, 손실, 옵티마이저 생성\n",
    "dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = SurnameClassifier(input_dim=len(vectorizer.surname_vocab), \n",
    "                               hidden_dim=args.hidden_dim, \n",
    "                               output_dim=len(vectorizer.nationality_vocab))\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 반복 코드의 일부\n",
    "# 훈련 과정은 5단계입니다\n",
    "\n",
    "# --------------------------------------\n",
    "# 단계 1. 그레이디언트를 0으로 초기화합니다\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 단계 2. 출력을 계산합니다\n",
    "y_pred = classifier(batch_dict['x_surname'])\n",
    "\n",
    "# 단계 3. 손실을 계산합니다\n",
    "loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "loss_t = loss.item()\n",
    "running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "# 단계 4. 손실을 사용해 그레이디언트를 계산합니다\n",
    "loss.backward()\n",
    "\n",
    "# 단계 5. 옵티마이저로 가중치를 업데이트합니다\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91296f0",
   "metadata": {},
   "source": [
    "4.2.5 모델 평가와 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd724e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 모델(분류기)을 사용한 추론: 주어진 이름의 국적 예측하기\n",
    "def predict_nationality(surname, classifier, vectorizer):\n",
    "    vectorized_surname = vectorizer.vectorize(surname)\n",
    "    vectorized_surname = torch.tensor(vectorized_surname).view(1, -1)\n",
    "    result = classifier(vectorized_surname, apply_softmax=True)\n",
    "\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    index = indices.item()\n",
    "\n",
    "    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "    probability_value = probability_values.item()\n",
    "\n",
    "    return {'nationality': predicted_nationality, 'probability': probability_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0025b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최상위 국적 k개 예측하기\n",
    "def predict_topk_nationality(name, classifier, vectorizer, k=5):\n",
    "    vectorized_name = vectorizer.vectorize(name)\n",
    "    vectorized_name = torch.tensor(vectorized_name).view(1, -1)\n",
    "    prediction_vector = classifier(vectorized_name, apply_softmax=True)\n",
    "    probability_values, indices = torch.topk(prediction_vector, k=k)\n",
    "    \n",
    "    # 반환되는 크기는 (1,k)입니다\n",
    "    probability_values = probability_values.detach().numpy()[0]\n",
    "    indices = indices.detach().numpy()[0]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for prob_value, index in zip(probability_values, indices):\n",
    "        nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "        results.append({'nationality': nationality, \n",
    "                        'probability': prob_value})\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28929f1",
   "metadata": {},
   "source": [
    "4.2.6 MLP 규제: 가중치 규제와 구조적 규제(또는 드롭아웃)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab30c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드롭아웃을 적용한 MLP\n",
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            input_dim (int): 입력 벡터 크기\n",
    "            hidden_dim (int): 첫 번째 Linear 층의 출력 크기\n",
    "            output_dim (int): 두 번째 Linear 층의 출력 크기\n",
    "        \"\"\"\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"MLP의 정방향 계산\n",
    "        \n",
    "        매개변수:\n",
    "            x_in (torch.Tensor): 입력 데이터 텐서\n",
    "                x_in.shape는 (batch, input_dim)입니다.\n",
    "            apply_softmax (bool): 소프트맥스 활성화 함수를 위한 플래그\n",
    "                크로스-엔트로피 손실을 사용하려면 False로 지정해야 합니다.\n",
    "        반환값:\n",
    "            결과 텐서. tensor.shape은 (batch, output_dim)입니다.\n",
    "        \"\"\"\n",
    "        intermediate = F.relu(self.fc1(x_in))\n",
    "        output = self.fc2(F.dropout(intermediate, p=0.5))\n",
    "        \n",
    "        if apply_softmax:\n",
    "            output = F.softmax(output, dim=1)\n",
    "        return output\n",
    "\n",
    "batch_size = 2 # 동시에 입력될 샘플의 개수\n",
    "input_dim = 3\n",
    "hidden_dim = 100\n",
    "output_dim = 4\n",
    "\n",
    "# 모델 생성\n",
    "mlp = MultilayerPerceptron(input_dim, hidden_dim, output_dim)\n",
    "print(mlp)\n",
    "\n",
    "y_output = mlp(x_input, apply_softmax=False)\n",
    "describe(y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60154dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
