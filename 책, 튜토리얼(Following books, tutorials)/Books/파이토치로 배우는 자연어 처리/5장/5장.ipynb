{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5장",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "5.1.2 단어 임베딩 방법"
      ],
      "metadata": {
        "id": "-7GOsVf5QV_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install annoy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ecQ50QGQkMM",
        "outputId": "3d0612e1-1739-4e07-d024-fe838646c87d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting annoy\n",
            "  Downloading annoy-1.17.0.tar.gz (646 kB)\n",
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████                            | 81 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 317 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 327 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 337 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 348 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 358 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 368 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 378 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 389 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 399 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 409 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 419 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 430 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 440 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 450 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 460 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 471 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 481 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 491 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 501 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 512 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 522 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 532 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 542 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 552 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 563 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 573 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 583 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 593 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 604 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 614 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 624 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 634 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 646 kB 5.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=391683 sha256=566c57038f514f10cd8768bccd85c50222deca787690121d34b836ebf1744230\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/e8/1e/7cc9ebbfa87a3b9f8ba79408d4d31831d67eea918b679a4c07\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from annoy import AnnoyIndex\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ZCUv9kMYQiNh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEhE8iXCQROl"
      },
      "outputs": [],
      "source": [
        "# 사전 훈련된 단어 임베딩 사용하기\n",
        "class PreTrainedEmbeddings(object):\n",
        "    \"\"\" 사전 훈련된 단어 벡터 사용을 위한 래퍼 클래스 \"\"\"\n",
        "    def __init__(self, word_to_index, word_vectors):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            word_to_index (dict): 단어에서 정수로 매핑\n",
        "            word_vectors (numpy 배열의 리스트)\n",
        "        \"\"\"\n",
        "        self.word_to_index = word_to_index\n",
        "        self.word_vectors = word_vectors\n",
        "        self.index_to_word = {v: k for k, v in self.word_to_index.items()}\n",
        "\n",
        "        self.index = AnnoyIndex(len(word_vectors[0]), metric='euclidean')\n",
        "        print(\"인덱스 만드는 중!\")\n",
        "        for _, i in self.word_to_index.items():\n",
        "            self.index.add_item(i, self.word_vectors[i])\n",
        "        self.index.build(50)\n",
        "        print(\"완료!\")\n",
        "        \n",
        "    @classmethod\n",
        "    def from_embeddings_file(cls, embedding_file):\n",
        "        \"\"\"사전 훈련된 벡터 파일에서 객체를 만듭니다.\n",
        "        \n",
        "        벡터 파일은 다음과 같은 포맷입니다:\n",
        "            word0 x0_0 x0_1 x0_2 x0_3 ... x0_N\n",
        "            word1 x1_0 x1_1 x1_2 x1_3 ... x1_N\n",
        "        \n",
        "        매개변수:\n",
        "            embedding_file (str): 파일 위치\n",
        "        반환값:\n",
        "            PretrainedEmbeddings의 인스턴스\n",
        "        \"\"\"\n",
        "        word_to_index = {}\n",
        "        word_vectors = []\n",
        "\n",
        "        with open(embedding_file) as fp:\n",
        "            for line in fp.readlines():\n",
        "                line = line.split(\" \")\n",
        "                word = line[0]\n",
        "                vec = np.array([float(x) for x in line[1:]])\n",
        "                \n",
        "                word_to_index[word] = len(word_to_index)\n",
        "                word_vectors.append(vec)\n",
        "                \n",
        "        return cls(word_to_index, word_vectors)\n",
        "\n",
        "embeddings = PreTrainedEmbeddings.from_embeddings_file('data/glove/glove.6B.100d.txt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  단어 임베딩을 사용한 유추 작업\n",
        " class PreTrainedEmbeddings(object):\n",
        "   \"\"\"이전 코드에서 이어진 구현 \"\"\"\n",
        "   def get_embedding(self, word):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            word (str)\n",
        "        반환값\n",
        "            임베딩 (numpy.ndarray)\n",
        "        \"\"\"\n",
        "        return self.word_vectors[self.word_to_index[word]]\n",
        "    \n",
        "    def get_closest_to_vector(self, vector, n=1):\n",
        "        \"\"\"벡터가 주어지면 n 개의 최근접 이웃을 반환합니다\n",
        "        매개변수:\n",
        "            vector (np.ndarray): Annoy 인덱스에 있는 벡터의 크기와 같아야 합니다\n",
        "            n (int): 반환될 이웃의 개수\n",
        "        반환값:\n",
        "            [str, str, ...]: 주어진 벡터와 가장 가까운 단어\n",
        "                단어는 거리순으로 정렬되어 있지 않습니다.\n",
        "        \"\"\"\n",
        "        nn_indices = self.index.get_nns_by_vector(vector, n)\n",
        "        return [self.index_to_word[neighbor] for neighbor in nn_indices]\n",
        "    \n",
        "    def compute_and_print_analogy(self, word1, word2, word3):\n",
        "        \"\"\"단어 임베딩을 사용한 유추 결과를 출력합니다\n",
        "\n",
        "        word1이 word2일 때 word3은 __입니다.\n",
        "        이 메서드는 word1 : word2 :: word3 : word4를 출력합니다\n",
        "        \n",
        "        매개변수:\n",
        "            word1 (str)\n",
        "            word2 (str)\n",
        "            word3 (str)\n",
        "        \"\"\"\n",
        "        vec1 = self.get_embedding(word1)\n",
        "        vec2 = self.get_embedding(word2)\n",
        "        vec3 = self.get_embedding(word3)\n",
        "\n",
        "        # 네 번째 단어 임베딩을 계산합니다\n",
        "        spatial_relationship = vec2 - vec1\n",
        "        vec4 = vec3 + spatial_relationship\n",
        "\n",
        "        closest_words = self.get_closest_to_vector(vec4, n=4)\n",
        "        existing_words = set([word1, word2, word3])\n",
        "        closest_words = [word for word in closest_words \n",
        "                             if word not in existing_words] \n",
        "\n",
        "        if len(closest_words) == 0:\n",
        "            print(\"계산된 벡터와 가장 가까운 이웃을 찾을 수 없습니다!\")\n",
        "            return\n",
        "        \n",
        "        for word4 in closest_words:\n",
        "            print(\"{} : {} :: {} : {}\".format(word1, word2, word3, word4))"
      ],
      "metadata": {
        "id": "tXaBTfuRRINo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 완성된 코드\n",
        "class PreTrainedEmbeddings(object):\n",
        "    \"\"\" 사전 훈련된 단어 벡터 사용을 위한 래퍼 클래스 \"\"\"\n",
        "    def __init__(self, word_to_index, word_vectors):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            word_to_index (dict): 단어에서 정수로 매핑\n",
        "            word_vectors (numpy 배열의 리스트)\n",
        "        \"\"\"\n",
        "        self.word_to_index = word_to_index\n",
        "        self.word_vectors = word_vectors\n",
        "        self.index_to_word = {v: k for k, v in self.word_to_index.items()}\n",
        "\n",
        "        self.index = AnnoyIndex(len(word_vectors[0]), metric='euclidean')\n",
        "        print(\"인덱스 만드는 중!\")\n",
        "        for _, i in self.word_to_index.items():\n",
        "            self.index.add_item(i, self.word_vectors[i])\n",
        "        self.index.build(50)\n",
        "        print(\"완료!\")\n",
        "        \n",
        "    @classmethod\n",
        "    def from_embeddings_file(cls, embedding_file):\n",
        "        \"\"\"사전 훈련된 벡터 파일에서 객체를 만듭니다.\n",
        "        \n",
        "        벡터 파일은 다음과 같은 포맷입니다:\n",
        "            word0 x0_0 x0_1 x0_2 x0_3 ... x0_N\n",
        "            word1 x1_0 x1_1 x1_2 x1_3 ... x1_N\n",
        "        \n",
        "        매개변수:\n",
        "            embedding_file (str): 파일 위치\n",
        "        반환값:\n",
        "            PretrainedEmbeddings의 인스턴스\n",
        "        \"\"\"\n",
        "        word_to_index = {}\n",
        "        word_vectors = []\n",
        "\n",
        "        with open(embedding_file) as fp:\n",
        "            for line in fp.readlines():\n",
        "                line = line.split(\" \")\n",
        "                word = line[0]\n",
        "                vec = np.array([float(x) for x in line[1:]])\n",
        "                \n",
        "                word_to_index[word] = len(word_to_index)\n",
        "                word_vectors.append(vec)\n",
        "                \n",
        "        return cls(word_to_index, word_vectors)\n",
        "    \n",
        "    def get_embedding(self, word):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            word (str)\n",
        "        반환값\n",
        "            임베딩 (numpy.ndarray)\n",
        "        \"\"\"\n",
        "        return self.word_vectors[self.word_to_index[word]]\n",
        "\n",
        "    def get_closest_to_vector(self, vector, n=1):\n",
        "        \"\"\"벡터가 주어지면 n 개의 최근접 이웃을 반환합니다\n",
        "        매개변수:\n",
        "            vector (np.ndarray): Annoy 인덱스에 있는 벡터의 크기와 같아야 합니다\n",
        "            n (int): 반환될 이웃의 개수\n",
        "        반환값:\n",
        "            [str, str, ...]: 주어진 벡터와 가장 가까운 단어\n",
        "                단어는 거리순으로 정렬되어 있지 않습니다.\n",
        "        \"\"\"\n",
        "        nn_indices = self.index.get_nns_by_vector(vector, n)\n",
        "        return [self.index_to_word[neighbor] for neighbor in nn_indices]\n",
        "    \n",
        "    def compute_and_print_analogy(self, word1, word2, word3):\n",
        "        \"\"\"단어 임베딩을 사용한 유추 결과를 출력합니다\n",
        "\n",
        "        word1이 word2일 때 word3은 __입니다.\n",
        "        이 메서드는 word1 : word2 :: word3 : word4를 출력합니다\n",
        "        \n",
        "        매개변수:\n",
        "            word1 (str)\n",
        "            word2 (str)\n",
        "            word3 (str)\n",
        "        \"\"\"\n",
        "        vec1 = self.get_embedding(word1)\n",
        "        vec2 = self.get_embedding(word2)\n",
        "        vec3 = self.get_embedding(word3)\n",
        "\n",
        "        # 네 번째 단어 임베딩을 계산합니다\n",
        "        spatial_relationship = vec2 - vec1\n",
        "        vec4 = vec3 + spatial_relationship\n",
        "\n",
        "        closest_words = self.get_closest_to_vector(vec4, n=4)\n",
        "        existing_words = set([word1, word2, word3])\n",
        "        closest_words = [word for word in closest_words \n",
        "                             if word not in existing_words] \n",
        "\n",
        "        if len(closest_words) == 0:\n",
        "            print(\"계산된 벡터와 가장 가까운 이웃을 찾을 수 없습니다!\")\n",
        "            return\n",
        "        \n",
        "        for word4 in closest_words:\n",
        "            print(\"{} : {} :: {} : {}\".format(word1, word2, word3, word4))\n",
        "# data/glove/glove.6B.100d.txt\n",
        "embeddings = PreTrainedEmbeddings.from_embeddings_file('/content/drive/MyDrive/glove.6B/glove.6B.100d.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO-nGgraTvXI",
        "outputId": "0d567ac8-f0b8-4c5e-977b-e9f213f614cf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인덱스 만드는 중!\n",
            "완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SAT 유추 작업에서 봤듯이 단어 임베딩은 많은 언어 관계를 인코딩합니다\n",
        "# 관계 1 : 성별 명사와 대명사의 관계\n",
        "print(embeddings.compute_and_print_analogy('man', 'he', 'woman'))\n",
        "\n",
        "# 관계 2 : 동사-명사 관계\n",
        "print(embeddings.compute_and_print_analogy('fly', 'plane', 'sail'))\n",
        "\n",
        "# 관계 3 : 명사-명사 관계\n",
        "print(embeddings.compute_and_print_analogy('cat', 'kitten', 'dog'))\n",
        "\n",
        "# 관계 4 : 상위어(Hypernymy) (더 넓은 범주)\n",
        "print(embeddings.compute_and_print_analogy('blue', 'color', 'dog'))\n",
        "\n",
        "# 관계 5 : 부분에서 전체(Meronymy)\n",
        "print(embeddings.compute_and_print_analogy('toe', 'foot', 'finger'))\n",
        "\n",
        "# 관계 6 : 방식 차이(Troponymy)\n",
        "print(embeddings.compute_and_print_analogy('talk', 'communicate', 'read'))\n",
        "\n",
        "# 관계 7 : 전체 의미 표현(Mentonymy) (관습 / 인물)\n",
        "print(embeddings.compute_and_print_analogy('blue', 'democrat', 'red'))\n",
        "\n",
        "# 관계 8 : 비교급\n",
        "print(embeddings.compute_and_print_analogy('fast', 'fastest', 'young'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n13sySy4RIQY",
        "outputId": "98d278a3-1f3e-4071-dd65-9bb305efdf6e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "man : he :: woman : she\n",
            "man : he :: woman : her\n",
            "None\n",
            "fly : plane :: sail : ship\n",
            "fly : plane :: sail : vessel\n",
            "fly : plane :: sail : boat\n",
            "None\n",
            "cat : kitten :: dog : puppy\n",
            "cat : kitten :: dog : puppies\n",
            "cat : kitten :: dog : junkyard\n",
            "None\n",
            "blue : color :: dog : animal\n",
            "blue : color :: dog : pet\n",
            "blue : color :: dog : taste\n",
            "blue : color :: dog : touch\n",
            "None\n",
            "toe : foot :: finger : hand\n",
            "toe : foot :: finger : kept\n",
            "toe : foot :: finger : ground\n",
            "None\n",
            "talk : communicate :: read : interpret\n",
            "talk : communicate :: read : communicated\n",
            "talk : communicate :: read : transmit\n",
            "None\n",
            "blue : democrat :: red : republican\n",
            "blue : democrat :: red : congressman\n",
            "blue : democrat :: red : senator\n",
            "None\n",
            "fast : fastest :: young : younger\n",
            "fast : fastest :: young : sixth\n",
            "fast : fastest :: young : fifth\n",
            "fast : fastest :: young : seventh\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 동시에 등장하는 정보로 의미를 인코딩하는 위험을 보여주는 예. 항상 이렇지는 않음\n",
        "embeddings.compute_and_print_analogy('fast', 'fastest', 'small')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo5AxTdbRITP",
        "outputId": "97afb7ad-9527-40b3-b9b4-066480051821"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fast : fastest :: small : smallest\n",
            "fast : fastest :: small : largest\n",
            "fast : fastest :: small : among\n",
            "fast : fastest :: small : quarters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 임베딩에 인코딩된 성별과 같은 보호 속성에 주의. 이로 인해 하위 모델에서 원치 않는 편향 발생 가능\n",
        "embeddings.compute_and_print_analogy('man', 'king', 'woman')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uaBYwrkRIWw",
        "outputId": "cee9c3bd-01b7-4c46-9825-cdad6c93bea1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "man : king :: woman : queen\n",
            "man : king :: woman : monarch\n",
            "man : king :: woman : throne\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 벡터에 인코딩된 문화적 성별 편견\n",
        "embeddings.compute_and_print_analogy('man', 'doctor', 'woman')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_EWoWseRIZs",
        "outputId": "89a865c5-8d2d-481e-8536-8e1b165185ac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "man : doctor :: woman : nurse\n",
            "man : doctor :: woman : physician\n",
            "man : doctor :: woman : doctors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.2.1 프랑켄슈타인 데이터셋"
      ],
      "metadata": {
        "id": "oSROIpixW0d6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CBOW 작업을 위한 데이터셋 클래스\n",
        "class CBOWDataset(Dataset):\n",
        "  @classmethod\n",
        "  def load_dataset_and_make_vectorizer(cls, cbow_csv):\n",
        "      \"\"\"데이터셋을 로드하고 처음부터 새로운 Vectorizer 만들기\n",
        "      \n",
        "      매개변수:\n",
        "          cbow_csv (str): 데이터셋의 위치\n",
        "      반환값:\n",
        "          CBOWDataset의 인스턴스\n",
        "      \"\"\"\n",
        "      cbow_df = pd.read_csv(cbow_csv)\n",
        "      train_cbow_df = cbow_df[cbow_df.split=='train']\n",
        "      return cls(cbow_df, CBOWVectorizer.from_dataframe(train_cbow_df))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        \"\"\"파이토치 데이터셋의 주요 진입 메서드\n",
        "        \n",
        "        매개변수:\n",
        "            index (int): 데이터 포인트의 인덱스\n",
        "        반환값:\n",
        "            데이터 포인트의 특성(x_data)과 레이블(y_target)로 이루어진 딕셔너리\n",
        "        \"\"\"\n",
        "        row = self._target_df.iloc[index]\n",
        "\n",
        "        context_vector = \\\n",
        "            self._vectorizer.vectorize(row.context, self._max_seq_length)\n",
        "        target_index = self._vectorizer.cbow_vocab.lookup_token(row.target)\n",
        "\n",
        "        return {'x_data': context_vector,\n",
        "                'y_target': target_index}"
      ],
      "metadata": {
        "id": "RsnaScmnRIgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z28HJOgyRIi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rw_YneriRIlu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}