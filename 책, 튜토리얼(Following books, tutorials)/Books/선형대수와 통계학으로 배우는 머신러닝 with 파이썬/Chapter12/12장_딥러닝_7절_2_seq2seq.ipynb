{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 시드 설정\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = 64                            # 배치 크기\n",
    "epochs = 100                            # 학습 epochs\n",
    "latent_dim = 256                        # 단어 인코딩 축소 차원 \n",
    "n_max_sample = 10000                    # 학습 시킬 최대 샘플 수\n",
    "data_path = '../data/eng-fra/fra.txt'    # 데이터 텍스트 파일 경로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전체 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)',\n",
       " 'Hi.\\tSalut !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)',\n",
       " 'Hi.\\tSalut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)',\n",
       " 'Run!\\tCours\\u202f!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906331 (sacredceltic)',\n",
       " 'Run!\\tCourez\\u202f!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906332 (sacredceltic)',\n",
       " 'Who?\\tQui ?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) & #4366796 (gillux)',\n",
       " 'Wow!\\tÇa alors\\u202f!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) & #374631 (zmoo)',\n",
       " 'Fire!\\tAu feu !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1829639 (Spamster) & #4627939 (sacredceltic)',\n",
       " \"Help!\\tÀ l'aide\\u202f!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #435084 (lukaszpp) & #128430 (sysko)\",\n",
       " 'Jump.\\tSaute.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #631038 (Shishir) & #2416938 (Micsmithel)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인풋, 타겟 텍스트 데이터 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_txts = []\n",
    "y_txts = []\n",
    "x_chars_uni = set()\n",
    "y_chars_uni = set()\n",
    "n_sample = min(n_max_sample, len(lines) - 1)    \n",
    "\n",
    "for line in lines[:n_sample]:\n",
    "    x_txt, y_txt, _ = line.split('\\t')\n",
    "    y_txt = '\\t' + y_txt + '\\n'\n",
    "    x_txts.append(x_txt)\n",
    "    y_txts.append(y_txt)\n",
    "    \n",
    "    for char in x_txt:\n",
    "        if char not in x_chars_uni:\n",
    "            x_chars_uni.add(char)\n",
    "    for char in y_txt:\n",
    "        if char not in y_chars_uni:\n",
    "            y_chars_uni.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.', 'Hi.', 'Hi.', 'Run!', 'Run!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_txts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\tVa !\\n',\n",
       " '\\tSalut !\\n',\n",
       " '\\tSalut.\\n',\n",
       " '\\tCours\\u202f!\\n',\n",
       " '\\tCourez\\u202f!\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_txts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " '!',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'Y',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " 'é'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_chars_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t',\n",
       " '\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '5',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'Y',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '\\xa0',\n",
       " '«',\n",
       " '»',\n",
       " 'À',\n",
       " 'Ç',\n",
       " 'É',\n",
       " 'Ê',\n",
       " 'à',\n",
       " 'â',\n",
       " 'ç',\n",
       " 'è',\n",
       " 'é',\n",
       " 'ê',\n",
       " 'ë',\n",
       " 'î',\n",
       " 'ï',\n",
       " 'ô',\n",
       " 'ù',\n",
       " 'û',\n",
       " 'œ',\n",
       " '\\u2009',\n",
       " '’',\n",
       " '\\u202f'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_chars_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰 단위 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_chars_uni = sorted(list(x_chars_uni))\n",
    "y_chars_uni = sorted(list(y_chars_uni))\n",
    "n_encoder_tokens = len(x_chars_uni)\n",
    "n_decoder_tokens = len(y_chars_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유니크 인코더 토큰 글자 수:  71\n",
      "유니크 디코더 토큰 글자 수:  93\n"
     ]
    }
   ],
   "source": [
    "print(\"유니크 인코더 토큰 글자 수: \", n_encoder_tokens)\n",
    "print(\"유니크 디코더 토큰 글자 수: \", n_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더 문장내 최대 문자 수:  15\n"
     ]
    }
   ],
   "source": [
    "max_encoder_seq_len = 0\n",
    "\n",
    "for txt in x_txts:\n",
    "    txt_len = len(txt)\n",
    "    max_encoder_seq_len = max(txt_len, \n",
    "                              max_encoder_seq_len)\n",
    "    \n",
    "print(\"인코더 문장내 최대 문자 수: \", max_encoder_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디코더 문장내 최대 문자 수:  59\n"
     ]
    }
   ],
   "source": [
    "max_decoder_seq_len = 0\n",
    "\n",
    "for txt in y_txts:\n",
    "    txt_len = len(txt)\n",
    "    max_decoder_seq_len = max(txt_len, \n",
    "                              max_decoder_seq_len)\n",
    "    \n",
    "print(\"디코더 문장내 최대 문자 수: \", max_decoder_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰 별 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_token_idx = {}\n",
    "\n",
    "for idx, char in enumerate(x_chars_uni):\n",
    "    x_token_idx[char] = idx\n",
    "    \n",
    "y_token_idx ={}\n",
    "for idx, char in enumerate(y_chars_uni):\n",
    "    y_token_idx[char] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '!': 1,\n",
       " '$': 2,\n",
       " '%': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " '0': 9,\n",
       " '1': 10,\n",
       " '2': 11,\n",
       " '3': 12,\n",
       " '5': 13,\n",
       " '6': 14,\n",
       " '7': 15,\n",
       " '8': 16,\n",
       " '9': 17,\n",
       " ':': 18,\n",
       " '?': 19,\n",
       " 'A': 20,\n",
       " 'B': 21,\n",
       " 'C': 22,\n",
       " 'D': 23,\n",
       " 'E': 24,\n",
       " 'F': 25,\n",
       " 'G': 26,\n",
       " 'H': 27,\n",
       " 'I': 28,\n",
       " 'J': 29,\n",
       " 'K': 30,\n",
       " 'L': 31,\n",
       " 'M': 32,\n",
       " 'N': 33,\n",
       " 'O': 34,\n",
       " 'P': 35,\n",
       " 'Q': 36,\n",
       " 'R': 37,\n",
       " 'S': 38,\n",
       " 'T': 39,\n",
       " 'U': 40,\n",
       " 'V': 41,\n",
       " 'W': 42,\n",
       " 'Y': 43,\n",
       " 'a': 44,\n",
       " 'b': 45,\n",
       " 'c': 46,\n",
       " 'd': 47,\n",
       " 'e': 48,\n",
       " 'f': 49,\n",
       " 'g': 50,\n",
       " 'h': 51,\n",
       " 'i': 52,\n",
       " 'j': 53,\n",
       " 'k': 54,\n",
       " 'l': 55,\n",
       " 'm': 56,\n",
       " 'n': 57,\n",
       " 'o': 58,\n",
       " 'p': 59,\n",
       " 'q': 60,\n",
       " 'r': 61,\n",
       " 's': 62,\n",
       " 't': 63,\n",
       " 'u': 64,\n",
       " 'v': 65,\n",
       " 'w': 66,\n",
       " 'x': 67,\n",
       " 'y': 68,\n",
       " 'z': 69,\n",
       " 'é': 70}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_token_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t': 0,\n",
       " '\\n': 1,\n",
       " ' ': 2,\n",
       " '!': 3,\n",
       " '$': 4,\n",
       " '%': 5,\n",
       " '&': 6,\n",
       " \"'\": 7,\n",
       " '(': 8,\n",
       " ')': 9,\n",
       " ',': 10,\n",
       " '-': 11,\n",
       " '.': 12,\n",
       " '0': 13,\n",
       " '1': 14,\n",
       " '2': 15,\n",
       " '3': 16,\n",
       " '5': 17,\n",
       " '8': 18,\n",
       " '9': 19,\n",
       " ':': 20,\n",
       " '?': 21,\n",
       " 'A': 22,\n",
       " 'B': 23,\n",
       " 'C': 24,\n",
       " 'D': 25,\n",
       " 'E': 26,\n",
       " 'F': 27,\n",
       " 'G': 28,\n",
       " 'H': 29,\n",
       " 'I': 30,\n",
       " 'J': 31,\n",
       " 'K': 32,\n",
       " 'L': 33,\n",
       " 'M': 34,\n",
       " 'N': 35,\n",
       " 'O': 36,\n",
       " 'P': 37,\n",
       " 'Q': 38,\n",
       " 'R': 39,\n",
       " 'S': 40,\n",
       " 'T': 41,\n",
       " 'U': 42,\n",
       " 'V': 43,\n",
       " 'Y': 44,\n",
       " 'a': 45,\n",
       " 'b': 46,\n",
       " 'c': 47,\n",
       " 'd': 48,\n",
       " 'e': 49,\n",
       " 'f': 50,\n",
       " 'g': 51,\n",
       " 'h': 52,\n",
       " 'i': 53,\n",
       " 'j': 54,\n",
       " 'k': 55,\n",
       " 'l': 56,\n",
       " 'm': 57,\n",
       " 'n': 58,\n",
       " 'o': 59,\n",
       " 'p': 60,\n",
       " 'q': 61,\n",
       " 'r': 62,\n",
       " 's': 63,\n",
       " 't': 64,\n",
       " 'u': 65,\n",
       " 'v': 66,\n",
       " 'x': 67,\n",
       " 'y': 68,\n",
       " 'z': 69,\n",
       " '\\xa0': 70,\n",
       " '«': 71,\n",
       " '»': 72,\n",
       " 'À': 73,\n",
       " 'Ç': 74,\n",
       " 'É': 75,\n",
       " 'Ê': 76,\n",
       " 'à': 77,\n",
       " 'â': 78,\n",
       " 'ç': 79,\n",
       " 'è': 80,\n",
       " 'é': 81,\n",
       " 'ê': 82,\n",
       " 'ë': 83,\n",
       " 'î': 84,\n",
       " 'ï': 85,\n",
       " 'ô': 86,\n",
       " 'ù': 87,\n",
       " 'û': 88,\n",
       " 'œ': 89,\n",
       " '\\u2009': 90,\n",
       " '’': 91,\n",
       " '\\u202f': 92}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_token_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영행렬 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_x_data = np.zeros(\n",
    "                            (len(x_txts), \n",
    "                             max_encoder_seq_len, \n",
    "                             n_encoder_tokens),\n",
    "                        dtype='float32')\n",
    "decoder_x_data = np.zeros(\n",
    "                            (len(x_txts), \n",
    "                             max_decoder_seq_len, \n",
    "                             n_decoder_tokens),\n",
    "                        dtype='float32')\n",
    "decoder_y_data = np.zeros(\n",
    "                            (len(x_txts), \n",
    "                             max_decoder_seq_len, \n",
    "                             n_decoder_tokens),\n",
    "                        dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인풋 데이터 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x_txt in enumerate(x_txts):    \n",
    "    for t, char in enumerate(x_txt):\n",
    "        encoder_x_data[i, t, x_token_idx[char]] = 1.\n",
    "    encoder_x_data[i, t + 1:, x_token_idx[' ']] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 타겟 데이터 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, y_txt in enumerate(y_txts):       \n",
    "    for t, char in enumerate(y_txt):\n",
    "        decoder_x_data[i, t, y_token_idx[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_y_data[i, t - 1, y_token_idx[char]] = 1.\n",
    "            \n",
    "    decoder_x_data[i, t + 1:, y_token_idx[' ']] = 1.\n",
    "    decoder_y_data[i, t:, y_token_idx[' ']] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "\n",
    "encoder_inputs = Input(shape=(None, n_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 71)\n",
      "(None, 256)\n",
      "(None, 256)\n",
      "(None, 256)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_inputs.shape)\n",
    "print(encoder_outs.shape)\n",
    "print(state_h.shape)\n",
    "print(state_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, n_decoder_tokens))\n",
    "decoder = LSTM(latent_dim, \n",
    "                return_sequences=True, \n",
    "                return_state=True)\n",
    "decoder_outs, _, _ = decoder(decoder_inputs,\n",
    "                             initial_state=encoder_states)\n",
    "decoder_dense = TimeDistributed(Dense(n_decoder_tokens, \n",
    "                                      activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 93)\n",
      "(None, None, 93)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_inputs.shape)\n",
    "print(decoder_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인코더-디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None, 71)]   0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None, 93)]   0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 256),        335872      ['input_1[0][0]']                \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 256),  358400      ['input_2[0][0]',                \n",
      "                                 (None, 256),                     'lstm[0][1]',                   \n",
      "                                 (None, 256)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 93)    23901       ['lstm_1[0][0]']                 \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 718,173\n",
      "Trainable params: 718,173\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], \n",
    "              decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모형 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모형 적합 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 21s 155ms/step - loss: 1.1843 - accuracy: 0.7258 - val_loss: 1.0570 - val_accuracy: 0.7179\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 18s 148ms/step - loss: 0.8566 - accuracy: 0.7691 - val_loss: 0.8362 - val_accuracy: 0.7687\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 18s 148ms/step - loss: 0.6791 - accuracy: 0.8074 - val_loss: 0.7198 - val_accuracy: 0.7891\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.5935 - accuracy: 0.8270 - val_loss: 0.6498 - val_accuracy: 0.8093\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 18s 146ms/step - loss: 0.5451 - accuracy: 0.8405 - val_loss: 0.6116 - val_accuracy: 0.8197\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 18s 145ms/step - loss: 0.5084 - accuracy: 0.8509 - val_loss: 0.5822 - val_accuracy: 0.8275\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 18s 147ms/step - loss: 0.4794 - accuracy: 0.8583 - val_loss: 0.5613 - val_accuracy: 0.8332\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 18s 147ms/step - loss: 0.4552 - accuracy: 0.8651 - val_loss: 0.5397 - val_accuracy: 0.8405\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 18s 146ms/step - loss: 0.4344 - accuracy: 0.8707 - val_loss: 0.5188 - val_accuracy: 0.8470\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 18s 146ms/step - loss: 0.4152 - accuracy: 0.8762 - val_loss: 0.5097 - val_accuracy: 0.8490\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 19s 148ms/step - loss: 0.3980 - accuracy: 0.8810 - val_loss: 0.4881 - val_accuracy: 0.8550\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 18s 146ms/step - loss: 0.3822 - accuracy: 0.8854 - val_loss: 0.4847 - val_accuracy: 0.8569\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 18s 146ms/step - loss: 0.3675 - accuracy: 0.8894 - val_loss: 0.4727 - val_accuracy: 0.8596\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 19s 148ms/step - loss: 0.3533 - accuracy: 0.8936 - val_loss: 0.4647 - val_accuracy: 0.8616\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 18s 146ms/step - loss: 0.3402 - accuracy: 0.8973 - val_loss: 0.4635 - val_accuracy: 0.8624\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 18s 147ms/step - loss: 0.3278 - accuracy: 0.9009 - val_loss: 0.4532 - val_accuracy: 0.8662\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 0.3159 - accuracy: 0.9044 - val_loss: 0.4518 - val_accuracy: 0.8669\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 29s 235ms/step - loss: 0.3045 - accuracy: 0.9078 - val_loss: 0.4472 - val_accuracy: 0.8688\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 29s 230ms/step - loss: 0.2938 - accuracy: 0.9111 - val_loss: 0.4443 - val_accuracy: 0.8696\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 29s 231ms/step - loss: 0.2835 - accuracy: 0.9141 - val_loss: 0.4401 - val_accuracy: 0.8709\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 29s 232ms/step - loss: 0.2742 - accuracy: 0.9168 - val_loss: 0.4425 - val_accuracy: 0.8717\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 30s 237ms/step - loss: 0.2649 - accuracy: 0.9198 - val_loss: 0.4423 - val_accuracy: 0.8724\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 29s 229ms/step - loss: 0.2559 - accuracy: 0.9224 - val_loss: 0.4422 - val_accuracy: 0.8733\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 30s 238ms/step - loss: 0.2478 - accuracy: 0.9249 - val_loss: 0.4425 - val_accuracy: 0.8738\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 29s 231ms/step - loss: 0.2399 - accuracy: 0.9269 - val_loss: 0.4410 - val_accuracy: 0.8743\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 29s 230ms/step - loss: 0.2321 - accuracy: 0.9294 - val_loss: 0.4364 - val_accuracy: 0.8761\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 29s 231ms/step - loss: 0.2242 - accuracy: 0.9316 - val_loss: 0.4449 - val_accuracy: 0.8741\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 30s 239ms/step - loss: 0.2172 - accuracy: 0.9336 - val_loss: 0.4448 - val_accuracy: 0.8754\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 29s 233ms/step - loss: 0.2105 - accuracy: 0.9358 - val_loss: 0.4474 - val_accuracy: 0.8756\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 30s 236ms/step - loss: 0.2040 - accuracy: 0.9379 - val_loss: 0.4502 - val_accuracy: 0.8757\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 29s 232ms/step - loss: 0.1974 - accuracy: 0.9398 - val_loss: 0.4533 - val_accuracy: 0.8755\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 29s 234ms/step - loss: 0.1915 - accuracy: 0.9414 - val_loss: 0.4569 - val_accuracy: 0.8761\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 29s 230ms/step - loss: 0.1859 - accuracy: 0.9434 - val_loss: 0.4575 - val_accuracy: 0.8768\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 28s 226ms/step - loss: 0.1802 - accuracy: 0.9451 - val_loss: 0.4571 - val_accuracy: 0.8771\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 29s 229ms/step - loss: 0.1747 - accuracy: 0.9467 - val_loss: 0.4637 - val_accuracy: 0.8770\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 29s 233ms/step - loss: 0.1699 - accuracy: 0.9476 - val_loss: 0.4686 - val_accuracy: 0.8772\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 29s 233ms/step - loss: 0.1651 - accuracy: 0.9493 - val_loss: 0.4693 - val_accuracy: 0.8771\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 29s 228ms/step - loss: 0.1600 - accuracy: 0.9508 - val_loss: 0.4760 - val_accuracy: 0.8761\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 28s 221ms/step - loss: 0.1562 - accuracy: 0.9523 - val_loss: 0.4809 - val_accuracy: 0.8756\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 28s 225ms/step - loss: 0.1516 - accuracy: 0.9534 - val_loss: 0.4841 - val_accuracy: 0.8770\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 28s 221ms/step - loss: 0.1476 - accuracy: 0.9547 - val_loss: 0.4866 - val_accuracy: 0.8768\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 28s 225ms/step - loss: 0.1435 - accuracy: 0.9557 - val_loss: 0.4938 - val_accuracy: 0.8759\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 28s 221ms/step - loss: 0.1394 - accuracy: 0.9568 - val_loss: 0.5026 - val_accuracy: 0.8755\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 29s 231ms/step - loss: 0.1355 - accuracy: 0.9582 - val_loss: 0.5042 - val_accuracy: 0.8750\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 28s 227ms/step - loss: 0.1324 - accuracy: 0.9590 - val_loss: 0.5032 - val_accuracy: 0.8765\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 28s 225ms/step - loss: 0.1290 - accuracy: 0.9601 - val_loss: 0.5066 - val_accuracy: 0.8757\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 28s 226ms/step - loss: 0.1258 - accuracy: 0.9612 - val_loss: 0.5160 - val_accuracy: 0.8758\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 28s 226ms/step - loss: 0.1226 - accuracy: 0.9621 - val_loss: 0.5191 - val_accuracy: 0.8761\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 29s 230ms/step - loss: 0.1192 - accuracy: 0.9630 - val_loss: 0.5206 - val_accuracy: 0.8762\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 29s 229ms/step - loss: 0.1165 - accuracy: 0.9638 - val_loss: 0.5297 - val_accuracy: 0.8747\n",
      "Epoch 51/100\n",
      "125/125 [==============================] - 30s 236ms/step - loss: 0.1139 - accuracy: 0.9645 - val_loss: 0.5305 - val_accuracy: 0.8764\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 29s 234ms/step - loss: 0.1090 - accuracy: 0.9661 - val_loss: 0.5372 - val_accuracy: 0.8753\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 29s 232ms/step - loss: 0.1083 - accuracy: 0.9661 - val_loss: 0.5446 - val_accuracy: 0.8747\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 29s 230ms/step - loss: 0.1064 - accuracy: 0.9664 - val_loss: 0.5469 - val_accuracy: 0.8745\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 29s 234ms/step - loss: 0.1034 - accuracy: 0.9676 - val_loss: 0.5490 - val_accuracy: 0.8752\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 29s 229ms/step - loss: 0.1010 - accuracy: 0.9680 - val_loss: 0.5528 - val_accuracy: 0.8751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "125/125 [==============================] - 30s 239ms/step - loss: 0.0985 - accuracy: 0.9691 - val_loss: 0.5564 - val_accuracy: 0.8752\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 28s 226ms/step - loss: 0.0962 - accuracy: 0.9698 - val_loss: 0.5605 - val_accuracy: 0.8754\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 28s 228ms/step - loss: 0.0943 - accuracy: 0.9704 - val_loss: 0.5675 - val_accuracy: 0.8761\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 28s 227ms/step - loss: 0.0918 - accuracy: 0.9709 - val_loss: 0.5748 - val_accuracy: 0.8755\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 28s 223ms/step - loss: 0.0900 - accuracy: 0.9714 - val_loss: 0.5760 - val_accuracy: 0.8742\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 29s 229ms/step - loss: 0.0879 - accuracy: 0.9724 - val_loss: 0.5840 - val_accuracy: 0.8748\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 28s 226ms/step - loss: 0.0859 - accuracy: 0.9727 - val_loss: 0.5816 - val_accuracy: 0.8756\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 29s 229ms/step - loss: 0.0842 - accuracy: 0.9732 - val_loss: 0.5901 - val_accuracy: 0.8748\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 28s 223ms/step - loss: 0.0821 - accuracy: 0.9736 - val_loss: 0.5957 - val_accuracy: 0.8742\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 28s 228ms/step - loss: 0.0803 - accuracy: 0.9744 - val_loss: 0.5949 - val_accuracy: 0.8755\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 28s 223ms/step - loss: 0.0788 - accuracy: 0.9748 - val_loss: 0.6019 - val_accuracy: 0.8752\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 29s 234ms/step - loss: 0.0769 - accuracy: 0.9751 - val_loss: 0.6008 - val_accuracy: 0.8751\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 29s 230ms/step - loss: 0.0754 - accuracy: 0.9758 - val_loss: 0.6084 - val_accuracy: 0.8752\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 29s 232ms/step - loss: 0.0741 - accuracy: 0.9759 - val_loss: 0.6141 - val_accuracy: 0.8741\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 29s 233ms/step - loss: 0.0724 - accuracy: 0.9766 - val_loss: 0.6182 - val_accuracy: 0.8732\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 30s 237ms/step - loss: 0.0712 - accuracy: 0.9771 - val_loss: 0.6178 - val_accuracy: 0.8745\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 30s 238ms/step - loss: 0.0697 - accuracy: 0.9773 - val_loss: 0.6206 - val_accuracy: 0.8751\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 29s 234ms/step - loss: 0.0684 - accuracy: 0.9777 - val_loss: 0.6327 - val_accuracy: 0.8756\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 29s 235ms/step - loss: 0.0669 - accuracy: 0.9781 - val_loss: 0.6331 - val_accuracy: 0.8739\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 29s 233ms/step - loss: 0.0656 - accuracy: 0.9786 - val_loss: 0.6382 - val_accuracy: 0.8734\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 30s 238ms/step - loss: 0.0641 - accuracy: 0.9788 - val_loss: 0.6435 - val_accuracy: 0.8736\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 29s 235ms/step - loss: 0.0630 - accuracy: 0.9791 - val_loss: 0.6442 - val_accuracy: 0.8747\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 30s 240ms/step - loss: 0.0620 - accuracy: 0.9795 - val_loss: 0.6487 - val_accuracy: 0.8737\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 28s 226ms/step - loss: 0.0609 - accuracy: 0.9799 - val_loss: 0.6533 - val_accuracy: 0.8738\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 29s 229ms/step - loss: 0.0595 - accuracy: 0.9801 - val_loss: 0.6562 - val_accuracy: 0.8729\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 29s 228ms/step - loss: 0.0589 - accuracy: 0.9803 - val_loss: 0.6613 - val_accuracy: 0.8737\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 28s 228ms/step - loss: 0.0578 - accuracy: 0.9808 - val_loss: 0.6708 - val_accuracy: 0.8727\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 28s 226ms/step - loss: 0.0567 - accuracy: 0.9812 - val_loss: 0.6712 - val_accuracy: 0.8729\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 29s 229ms/step - loss: 0.0555 - accuracy: 0.9813 - val_loss: 0.6745 - val_accuracy: 0.8726\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 29s 230ms/step - loss: 0.0548 - accuracy: 0.9818 - val_loss: 0.6744 - val_accuracy: 0.8726\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 19s 155ms/step - loss: 0.0537 - accuracy: 0.9817 - val_loss: 0.6810 - val_accuracy: 0.8732\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 19s 152ms/step - loss: 0.0527 - accuracy: 0.9821 - val_loss: 0.6883 - val_accuracy: 0.8728\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 19s 150ms/step - loss: 0.0519 - accuracy: 0.9824 - val_loss: 0.6850 - val_accuracy: 0.8730\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 19s 150ms/step - loss: 0.0511 - accuracy: 0.9826 - val_loss: 0.6991 - val_accuracy: 0.8732\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 19s 153ms/step - loss: 0.0503 - accuracy: 0.9829 - val_loss: 0.6931 - val_accuracy: 0.8731\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.0495 - accuracy: 0.9830 - val_loss: 0.6948 - val_accuracy: 0.8737\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 18s 148ms/step - loss: 0.0486 - accuracy: 0.9835 - val_loss: 0.6999 - val_accuracy: 0.8727\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 18s 147ms/step - loss: 0.0483 - accuracy: 0.9836 - val_loss: 0.7002 - val_accuracy: 0.8725\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 18s 147ms/step - loss: 0.0473 - accuracy: 0.9840 - val_loss: 0.7030 - val_accuracy: 0.8722\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 18s 148ms/step - loss: 0.0468 - accuracy: 0.9838 - val_loss: 0.7052 - val_accuracy: 0.8730\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 18s 147ms/step - loss: 0.0460 - accuracy: 0.9840 - val_loss: 0.7133 - val_accuracy: 0.8726\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 18s 147ms/step - loss: 0.0452 - accuracy: 0.9843 - val_loss: 0.7084 - val_accuracy: 0.8724\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 18s 147ms/step - loss: 0.0450 - accuracy: 0.9846 - val_loss: 0.7109 - val_accuracy: 0.8727\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 18s 148ms/step - loss: 0.0445 - accuracy: 0.9846 - val_loss: 0.7252 - val_accuracy: 0.8720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x199faec5040>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_x_data, decoder_x_data], decoder_y_data,\n",
    "          batch_size=n_batch,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 추론 샘플링 모형 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, \n",
    "                         decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 리버스 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_x_char_idx = {}\n",
    "\n",
    "for char, idx in x_token_idx.items():\n",
    "    reverse_x_char_idx[idx] = char\n",
    "    \n",
    "reverse_y_char_idx ={}\n",
    "\n",
    "for char, idx in y_token_idx.items():\n",
    "    reverse_y_char_idx[idx] = char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ' ',\n",
       " 1: '!',\n",
       " 2: '$',\n",
       " 3: '%',\n",
       " 4: '&',\n",
       " 5: \"'\",\n",
       " 6: ',',\n",
       " 7: '-',\n",
       " 8: '.',\n",
       " 9: '0',\n",
       " 10: '1',\n",
       " 11: '2',\n",
       " 12: '3',\n",
       " 13: '5',\n",
       " 14: '6',\n",
       " 15: '7',\n",
       " 16: '8',\n",
       " 17: '9',\n",
       " 18: ':',\n",
       " 19: '?',\n",
       " 20: 'A',\n",
       " 21: 'B',\n",
       " 22: 'C',\n",
       " 23: 'D',\n",
       " 24: 'E',\n",
       " 25: 'F',\n",
       " 26: 'G',\n",
       " 27: 'H',\n",
       " 28: 'I',\n",
       " 29: 'J',\n",
       " 30: 'K',\n",
       " 31: 'L',\n",
       " 32: 'M',\n",
       " 33: 'N',\n",
       " 34: 'O',\n",
       " 35: 'P',\n",
       " 36: 'Q',\n",
       " 37: 'R',\n",
       " 38: 'S',\n",
       " 39: 'T',\n",
       " 40: 'U',\n",
       " 41: 'V',\n",
       " 42: 'W',\n",
       " 43: 'Y',\n",
       " 44: 'a',\n",
       " 45: 'b',\n",
       " 46: 'c',\n",
       " 47: 'd',\n",
       " 48: 'e',\n",
       " 49: 'f',\n",
       " 50: 'g',\n",
       " 51: 'h',\n",
       " 52: 'i',\n",
       " 53: 'j',\n",
       " 54: 'k',\n",
       " 55: 'l',\n",
       " 56: 'm',\n",
       " 57: 'n',\n",
       " 58: 'o',\n",
       " 59: 'p',\n",
       " 60: 'q',\n",
       " 61: 'r',\n",
       " 62: 's',\n",
       " 63: 't',\n",
       " 64: 'u',\n",
       " 65: 'v',\n",
       " 66: 'w',\n",
       " 67: 'x',\n",
       " 68: 'y',\n",
       " 69: 'z',\n",
       " 70: 'é'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_x_char_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\t',\n",
       " 1: '\\n',\n",
       " 2: ' ',\n",
       " 3: '!',\n",
       " 4: '$',\n",
       " 5: '%',\n",
       " 6: '&',\n",
       " 7: \"'\",\n",
       " 8: '(',\n",
       " 9: ')',\n",
       " 10: ',',\n",
       " 11: '-',\n",
       " 12: '.',\n",
       " 13: '0',\n",
       " 14: '1',\n",
       " 15: '2',\n",
       " 16: '3',\n",
       " 17: '5',\n",
       " 18: '8',\n",
       " 19: '9',\n",
       " 20: ':',\n",
       " 21: '?',\n",
       " 22: 'A',\n",
       " 23: 'B',\n",
       " 24: 'C',\n",
       " 25: 'D',\n",
       " 26: 'E',\n",
       " 27: 'F',\n",
       " 28: 'G',\n",
       " 29: 'H',\n",
       " 30: 'I',\n",
       " 31: 'J',\n",
       " 32: 'K',\n",
       " 33: 'L',\n",
       " 34: 'M',\n",
       " 35: 'N',\n",
       " 36: 'O',\n",
       " 37: 'P',\n",
       " 38: 'Q',\n",
       " 39: 'R',\n",
       " 40: 'S',\n",
       " 41: 'T',\n",
       " 42: 'U',\n",
       " 43: 'V',\n",
       " 44: 'Y',\n",
       " 45: 'a',\n",
       " 46: 'b',\n",
       " 47: 'c',\n",
       " 48: 'd',\n",
       " 49: 'e',\n",
       " 50: 'f',\n",
       " 51: 'g',\n",
       " 52: 'h',\n",
       " 53: 'i',\n",
       " 54: 'j',\n",
       " 55: 'k',\n",
       " 56: 'l',\n",
       " 57: 'm',\n",
       " 58: 'n',\n",
       " 59: 'o',\n",
       " 60: 'p',\n",
       " 61: 'q',\n",
       " 62: 'r',\n",
       " 63: 's',\n",
       " 64: 't',\n",
       " 65: 'u',\n",
       " 66: 'v',\n",
       " 67: 'x',\n",
       " 68: 'y',\n",
       " 69: 'z',\n",
       " 70: '\\xa0',\n",
       " 71: '«',\n",
       " 72: '»',\n",
       " 73: 'À',\n",
       " 74: 'Ç',\n",
       " 75: 'É',\n",
       " 76: 'Ê',\n",
       " 77: 'à',\n",
       " 78: 'â',\n",
       " 79: 'ç',\n",
       " 80: 'è',\n",
       " 81: 'é',\n",
       " 82: 'ê',\n",
       " 83: 'ë',\n",
       " 84: 'î',\n",
       " 85: 'ï',\n",
       " 86: 'ô',\n",
       " 87: 'ù',\n",
       " 88: 'û',\n",
       " 89: 'œ',\n",
       " 90: '\\u2009',\n",
       " 91: '’',\n",
       " 92: '\\u202f'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_y_char_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과값 디코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    y_seq = np.zeros((1, 1, n_decoder_tokens))\n",
    "    y_seq[0, 0, y_token_idx['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [y_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_y_char_idx[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        y_seq = np.zeros((1, 1, n_decoder_tokens))\n",
    "        y_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Va !\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut.\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Courez !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Courez !\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Qui ?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Ça alors !\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Au feu !\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: À l'aide !\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Saute.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Stop !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Stop !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Stop !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attendez !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attendez !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuis.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuis.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuis.\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je comprends.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: J'essaye.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je l'ai emporté !\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je l'ai emporté !\n",
      "\n",
      "-\n",
      "Input sentence: I won.\n",
      "Decoded sentence: Je l'ai emporté !\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Oh non !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaquez !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaquez !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: À votre santé !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: À votre santé !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: À votre santé !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: À votre santé !\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Lève-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Vas-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Vas-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Vas-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris ?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris ?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris ?\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Sontez-toi !\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Sontez-toi !\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Serre-moi dans tes bras !\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Serre-moi dans tes bras !\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je suis tombée.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je suis tombée.\n",
      "\n",
      "-\n",
      "Input sentence: I fled.\n",
      "Decoded sentence: J'ai fui.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Je sais.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je suis partie.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je suis partie.\n",
      "\n",
      "-\n",
      "Input sentence: I lied.\n",
      "Decoded sentence: J'ai menti.\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: J'ai perdu.\n",
      "\n",
      "-\n",
      "Input sentence: I paid.\n",
      "Decoded sentence: J’ai payé.\n",
      "\n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Decoded sentence: J'ai foupe.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je suis en train de mentir.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je suis en train de mentir.\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: Écoutez !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Vraiment ?\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Vraiment ?\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Vraiment ?\n",
      "\n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Decoded sentence: Merci.\n",
      "\n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Decoded sentence: Merci.\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: On essaye.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous gagnâmes.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous gagnâmes.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous gagnâmes.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous gagnâmes.\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: Demande à Tom.\n",
      "\n",
      "-\n",
      "Input sentence: Awesome!\n",
      "Decoded sentence: Fantastique !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez calme !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez calme !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez calme !\n",
      "\n",
      "-\n",
      "Input sentence: Be cool.\n",
      "Decoded sentence: Sois détendu !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitable !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitable !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitable !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitable !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitable !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitable !\n",
      "\n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: Sois gentil.\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Dégage !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Appelle-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Appelle-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Appelle-nous !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_idx in range(100):\n",
    "    x_seq = encoder_x_data[seq_idx: seq_idx + 1]\n",
    "    decoded_sentence = decode_sequence(x_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', x_txts[seq_idx])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통합 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/eng-fra/fra.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-7f9e60a7546c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# 데이터 불러오기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/eng-fra/fra.txt'"
     ]
    }
   ],
   "source": [
    "# 랜덤 시드 설정\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "n_batch = 64                            # 배치 크기\n",
    "epochs = 100                            # 학습 epochs\n",
    "latent_dim = 256                        # 단어 인코딩 축소 차원 \n",
    "n_max_sample = 10000                    # 학습 시킬 최대 샘플 수\n",
    "data_path = '../data/eng-fra/fra.txt'    # 데이터 텍스트 파일 경로\n",
    "\n",
    "# 데이터 불러오기\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    \n",
    "# 인풋, 타겟 데이터 정리\n",
    "x_txts = []\n",
    "y_txts = []\n",
    "x_chars_uni = set()\n",
    "y_chars_uni = set()\n",
    "n_sample = min(n_max_sample, len(lines) - 1)    \n",
    "\n",
    "for line in lines[:n_sample]:\n",
    "    x_txt, y_txt, _ = line.split('\\t')\n",
    "    y_txt = '\\t' + y_txt + '\\n'\n",
    "    x_txts.append(x_txt)\n",
    "    y_txts.append(y_txt)\n",
    "    \n",
    "    for char in x_txt:\n",
    "        if char not in x_chars_uni:\n",
    "            x_chars_uni.add(char)\n",
    "    for char in y_txt:\n",
    "        if char not in y_chars_uni:\n",
    "            y_chars_uni.add(char)\n",
    "\n",
    "# 토큰 정리\n",
    "x_chars_uni = sorted(list(x_chars_uni))\n",
    "y_chars_uni = sorted(list(y_chars_uni))\n",
    "n_encoder_tokens = len(x_chars_uni)\n",
    "n_decoder_tokens = len(y_chars_uni)\n",
    "print(\"유니크 인코더 토큰 글자 수: \", n_encoder_tokens)\n",
    "print(\"유니크 디코더 토큰 글자 수: \", n_decoder_tokens)\n",
    "\n",
    "max_encoder_seq_len = 0\n",
    "for txt in x_txts:\n",
    "    txt_len = len(txt)\n",
    "    max_encoder_seq_len = max(txt_len, \n",
    "                              max_encoder_seq_len)\n",
    "print(\"인코더 문장내 최대 문자 수: \", max_encoder_seq_len)\n",
    "\n",
    "max_decoder_seq_len = 0\n",
    "for txt in y_txts:\n",
    "    txt_len = len(txt)\n",
    "    max_decoder_seq_len = max(txt_len, \n",
    "                              max_decoder_seq_len)\n",
    "print(\"디코더 문장내 최대 문자 수: \", max_decoder_seq_len)\n",
    "\n",
    "# 토큰 인덱스\n",
    "x_token_idx = {}\n",
    "for idx, char in enumerate(x_chars_uni):\n",
    "    x_token_idx[char] = idx\n",
    "    \n",
    "y_token_idx ={}\n",
    "for idx, char in enumerate(y_chars_uni):\n",
    "    y_token_idx[char] = idx\n",
    "    \n",
    "# 영행렬 만들기\n",
    "encoder_x_data = np.zeros(\n",
    "                            (len(x_txts), \n",
    "                             max_encoder_seq_len, \n",
    "                             n_encoder_tokens),\n",
    "                        dtype='float32')\n",
    "decoder_x_data = np.zeros(\n",
    "                            (len(x_txts), \n",
    "                             max_decoder_seq_len, \n",
    "                             n_decoder_tokens),\n",
    "                        dtype='float32')\n",
    "decoder_y_data = np.zeros(\n",
    "                            (len(x_txts), \n",
    "                             max_decoder_seq_len, \n",
    "                             n_decoder_tokens),\n",
    "                        dtype='float32')\n",
    "# 인풋 데이터 행렬\n",
    "for i, x_txt in enumerate(x_txts):\n",
    "    \n",
    "    for t, char in enumerate(x_txt):\n",
    "        encoder_x_data[i, t, x_token_idx[char]] = 1.\n",
    "    encoder_x_data[i, t + 1:, x_token_idx[' ']] = 1.\n",
    "\n",
    "# 타겟 데이터 행렬\n",
    "for i, y_txt in enumerate(y_txts):\n",
    "       \n",
    "    for t, char in enumerate(y_txt):\n",
    "        decoder_x_data[i, t, y_token_idx[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_y_data[i, t - 1, y_token_idx[char]] = 1.\n",
    "            \n",
    "    decoder_x_data[i, t + 1:, y_token_idx[' ']] = 1.\n",
    "    decoder_y_data[i, t:, y_token_idx[' ']] = 1.\n",
    "    \n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(None, n_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None, n_decoder_tokens))\n",
    "decoder = LSTM(latent_dim, \n",
    "                return_sequences=True, \n",
    "                return_state=True)\n",
    "decoder_outs, _, _ = decoder(decoder_inputs,\n",
    "                             initial_state=encoder_states)\n",
    "decoder_dense = TimeDistributed(Dense(n_decoder_tokens, \n",
    "                                      activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outs)\n",
    "\n",
    "# 인코더 디코더\n",
    "model = Model([encoder_inputs, decoder_inputs], \n",
    "              decoder_outputs)\n",
    "model.summary()\n",
    "\n",
    "# 모형 컴파일\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 학습\n",
    "model.fit([encoder_x_data, decoder_x_data], decoder_y_data,\n",
    "          batch_size=n_batch,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "# 추론 모형 생성\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, \n",
    "                         decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# 리버스 인덱스\n",
    "reverse_x_char_idx = {}\n",
    "for char, idx in x_token_idx.items():\n",
    "    reverse_x_char_idx[idx] = char\n",
    "    \n",
    "reverse_y_char_idx ={}\n",
    "for char, idx in y_token_idx.items():\n",
    "    reverse_y_char_idx[idx] = char\n",
    "\n",
    "# 결과값 디코딩\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    y_seq = np.zeros((1, 1, n_decoder_tokens))\n",
    "    y_seq[0, 0, y_token_idx['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [y_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_y_char_idx[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        y_seq = np.zeros((1, 1, n_decoder_tokens))\n",
    "        y_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "# 최종 결과 확인\n",
    "for seq_idx in range(100):\n",
    "    x_seq = encoder_x_data[seq_idx: seq_idx + 1]\n",
    "    decoded_sentence = decode_sequence(x_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', x_txts[seq_idx])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
